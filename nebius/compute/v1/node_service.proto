syntax = "proto3";

package nebius.compute.v1;

import "google/protobuf/timestamp.proto";
import "buf/validate/validate.proto";
import "nebius/annotations.proto";

option go_package = "github.com/nebius/gosdk/proto/nebius/compute/v1";
option java_multiple_files = true;
option java_outer_classname = "NodeServiceProto";
option java_package = "ai.nebius.pub.compute.v1";

service NodeService {
  option (api_service_name) = "compute";

  // SetUnhealthy marks the node underlying the Compute VM as unhealthy, which has the following effect:
  //
  // 1. Scheduler makes the best effort not to assign new VMs to the unhealthy node,
  //    but in case of no capacity, the VM can be assigned to an unhealthy node.
  // 2. The existing VMs continue to work on the node, but after stop/start via
  //    Compute API they most probably will be assigned to a different node.
  //
  // If the node was already marked unhealthy, the consecutive calls to SetUnhealthy
  // will return grpc code AlreadyExists.
  //
  // To use this rpc one needs to obtain `compute.node.setUnhealthy` permission
  // for the VM's parent container. The permission is granted to the TSA inside the VM.
  rpc SetUnhealthy(NodeSetUnhealthyRequest) returns (NodeSetUnhealthyResponse) {
    option (region_routing) = { nid: [ "instance_id" ] };
  }
}

message NodeSetUnhealthyRequest {
  message HealthCheckInfo {
    // Time when the unhealthy node was observed
    google.protobuf.Timestamp observed_at = 1 [(buf.validate.field).required = true];

    // Identifies specific GPU check that failed in soperator (key for observability)
    string check_id = 2 [(buf.validate.field).required = true];

    // Human-readable description of the error for further investigation
    string description = 3 [(buf.validate.field).required = true];
  }

  string instance_id = 1 [(buf.validate.field).required = true];

  HealthCheckInfo health_check_info = 2 [(buf.validate.field).required = true];
}

message NodeSetUnhealthyResponse {
}
