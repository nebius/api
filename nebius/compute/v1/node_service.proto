syntax = "proto3";

package nebius.compute.v1;

import "google/protobuf/timestamp.proto";
import "buf/validate/validate.proto";

option go_package = "github.com/nebius/gosdk/proto/nebius/compute/v1";
option java_multiple_files = true;
option java_outer_classname = "NodeServiceProto";
option java_package = "ai.nebius.pub.compute.v1";

service NodeService {
  // SetUnhealthy marks the node underlying the Compute VM as unhealthy,
  // which has the following effect:
  // 1. Scheduler makes its best effort not to assign new VMs to the unhealthy node,
  //    but in case of no capacity the VM can be assigned there.
  // 2. The existing VMs continue to work on the node, but after stop/start via
  //    Compute API they most probably will be assigned to different node (see 1.)
  //
  // If the node was already marked unhealthy, the consecutive calls to SetUnhealthy
  // will return grpc code AlreadyExists.
  //
  // To use this rpc one needs to obtain `compute.node.setUnhealthy` permission
  // for the VM's parent container. The permission is granted to the TSA inside the VM
  //
  rpc SetUnhealthy(NodeSetUnhealthyRequest) returns (NodeSetUnhealthyResponse);
}

message NodeSetUnhealthyRequest {
  message HealthCheckInfo {
    // Time when the unhealthy node was observed
    google.protobuf.Timestamp observed_at = 1 [(buf.validate.field).required = true];

    // Identifies specific GPU check that failed in soperator (key for observability)
    string check_id = 2 [(buf.validate.field).required = true];

    // Human-readable description of the error for further investigation
    string description = 3 [(buf.validate.field).required = true];
  }

  string instance_id = 1 [(buf.validate.field).required = true];

  HealthCheckInfo health_check_info = 2 [(buf.validate.field).required = true];
}

message NodeSetUnhealthyResponse {
}
