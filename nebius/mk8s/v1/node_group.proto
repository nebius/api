syntax = "proto3";

package nebius.mk8s.v1;

import "buf/validate/validate.proto";
import "google/protobuf/duration.proto";
import "nebius/annotations.proto";
import "nebius/common/v1/metadata.proto";
import "nebius/mk8s/v1/instance_template.proto";

option go_package = "github.com/nebius/gosdk/proto/nebius/mk8s/v1";
option java_multiple_files = true;
option java_outer_classname = "NodeGroupProto";
option java_package = "ai.nebius.pub.mk8s.v1";

// NodeGroup represents Kubernetes node pool - set of worker machines having the same configuration.
// A Node is a Nebius Compute Instance created in Cluster.metadata.parent_id container, running kubelet
// that registers in Kubernetes API and a Node object created.
message NodeGroup {
  common.v1.ResourceMetadata metadata = 1; // The parent_id is an ID of Cluster

  NodeGroupSpec spec = 2;

  NodeGroupStatus status = 3;
}

message NodeGroupSpec {
  // Version is desired Kubernetes version of the cluster. For now only acceptable format is
  // `<major>.<minor>` like "1.30". Option for patch version update will be added later.
  // By default the cluster control plane <major>.<minor> version will be used.
  string version = 1 [(buf.validate.field) = { string: { pattern: "|^\\d\\.\\d\\d$" } }];

  oneof size {
    option (buf.validate.oneof).required = true;

    // Number of nodes in the group. Can be changed manually at any time.
    int64 fixed_node_count = 2 [(buf.validate.field) = { int64: { lte: 100, gte: 0 } }];

    // Enables [Kubernetes Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)
    // for that NodeGroup, and defines autoscaling parameters.
    NodeGroupAutoscalingSpec autoscaling = 5;
  }

  // Parameters for Kubernetes Node object and Nebius Compute Instance
  // If not written opposite a NodeTemplate field update will cause NodeGroup roll-out according NodeGroupDeploymentStrategy.
  NodeTemplate template = 3 [(buf.validate.field).required = true];

  // Defines deployment - roll-out, or nodes re-creation during configuration change.
  // Allows to setup compromise in roll-out speed, extra resources consumption and workloads disruption.
  NodeGroupDeploymentStrategy strategy = 4;
}

message NodeTemplate {
  NodeMetadataTemplate metadata = 1;

  // Kubernetes Node taints.
  // For now change will not be propagated to existing nodes, so will be applied only to Kubernetes Nodes created after the field change.
  // That behaviour may change later.
  // So, for now you will need to manually set them to existing nodes, if that is needed.
  // Field change will NOT trigger NodeGroup roll out.
  repeated NodeTaint taints = 2 [(buf.validate.field) = { repeated: { max_items: 100 } }];

  // Resources that will have Nebius Compute Instance where Node kubelet will run.
  ResourcesSpec resources = 3 [(buf.validate.field).required = true];

  // Parameters of a Node Nebius Compute Instance boot disk.
  DiskSpec boot_disk = 9 [(field_behavior) = NON_EMPTY_DEFAULT];

  // Nebius Compute GPUCluster ID that will be attached to node.
  GpuClusterSpec gpu_cluster = 4;

  repeated NetworkInterfaceTemplate network_interfaces = 5 [(field_behavior) = NON_EMPTY_DEFAULT];

  // Static attachments of Compute Filesystem.
  // Can be used as a workaround, until CSI for Compute Disk and Filesystem will be available.
  repeated AttachedFilesystemSpec filesystems = 7;

  // cloud-init user-data
  // Should contain at least one SSH key.
  string cloud_init_user_data = 6 [(sensitive) = true];

  // the Nebius service account whose credentials will be available on the nodes of the group.
  // With these credentials, it is possible to make `npc` or public API requests from the nodes without the need for extra authentication.
  // This service account is also used to make requests to container registry.
  //
  // `resource.serviceaccount.issueAccessToken` permission is required to use this field.
  string service_account_id = 10;
}

message NodeMetadataTemplate {
  // Kubernetes Node labels.
  // For now change will not be propagated to existing nodes, so will be applied only to Kubernetes Nodes created after the field change.
  // That behaviour may change later.
  // So, for now you will need to manually set them to existing nodes, if that is needed.
  //
  // System labels containing "kubernetes.io" and "k8s.io" will be ignored.
  // Field change will NOT trigger NodeGroup roll out.
  map<string, string> labels = 1 [(buf.validate.field) = { map: { max_pairs: 100 } }];
}

message GpuClusterSpec {
  string id = 1;
}

message NetworkInterfaceTemplate {
  // Parameters for Public IPv4 address associated with the interface.
  // Set to empty value, to enable it.
  PublicIPAddress public_ip_address = 1 [(field_behavior) = MEANINGFUL_EMPTY_VALUE];

  // Nebius VPC Subnet ID that will be attached to a node cloud instance network interface.
  // By default Cluster control plane subnet_id used.
  // Subnet should be located in the same network with control plane.
  string subnet_id = 3 [(field_behavior) = NON_EMPTY_DEFAULT];
}

// Describes a public IP address.
message PublicIPAddress {
}

message AttachedFilesystemSpec {
  enum AttachMode {
    UNSPECIFIED = 0;

    READ_ONLY = 1;

    READ_WRITE = 2;
  }

  AttachMode attach_mode = 1 [(buf.validate.field).required = true];

  // Specifies the user-defined identifier, allowing to use it as a device in mount command.
  string mount_tag = 2 [(buf.validate.field).required = true];

  oneof type {
    option (buf.validate.oneof).required = true;

    ExistingFilesystem existing_filesystem = 3;
  }
}

message ExistingFilesystem {
  string id = 1 [(buf.validate.field).required = true];
}

message NodeGroupAutoscalingSpec {
  int64 min_node_count = 1 [(buf.validate.field) = { int64: { lte: 100, gte: 0 } }];

  int64 max_node_count = 2 [(buf.validate.field) = { int64: { lte: 100, gte: 0 } }];

  option (buf.validate.message) = {
    cel: [
      {
        id: "autoscaling.protovalidate.message",
        message: "min_node_count must be less or equal than max_node_count",
        expression: "this.min_node_count <= this.max_node_count"
      }
    ]
  };
}

// See https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
message NodeTaint {
  string key = 1 [(buf.validate.field).required = true];

  string value = 2 [(buf.validate.field).required = true];

  Effect effect = 3 [(buf.validate.field).required = true];

  enum Effect {
    EFFECT_UNSPECIFIED = 0;

    NO_EXECUTE = 1;

    NO_SCHEDULE = 2;

    PREFER_NO_SCHEDULE = 3;
  }
}

message NodeGroupDeploymentStrategy {
  // The maximum number of nodes that can be simultaneously unavailable during the update process.
  // This value can be specified either as an absolute number (for example 3) or as a percentage of the desired number of nodes (for example 5%).
  // When specified as a percentage, the actual number is calculated by rounding down to the nearest whole number.
  // This value cannot be 0 if `max_surge` is also set to 0.
  // Defaults to 0.
  // Example: If set to 20%, up to 20% of the nodes can be taken offline at once during the update,
  // ensuring that at least 80% of the desired nodes remain operational.
  PercentOrCount max_unavailable = 1;

  // The maximum number of additional nodes that can be provisioned above the desired number of nodes during the update process.
  // This value can be specified either as an absolute number (for example 3) or as a percentage of the desired number of nodes (for example 5%).
  // When specified as a percentage, the actual number is calculated by rounding up to the nearest whole number.
  // This value cannot be 0 if `max_unavailable` is also set to 0.
  // Defaults to 1.
  // Example: If set to 25%, the node group can scale up by an additional 25% during the update,
  // allowing new nodes to be added before old nodes are removed, which helps minimize workload disruption.
  // NOTE: it is user responsibility to ensure that there are enough quota for provision nodes above the desired number.
  //   Available quota effectively limits `max_surge`.
  //   In case of not enough quota even for one extra node, update operation will hung because of quota exhausted error.
  //   Such error will be visible in Operation.progress_data.
  PercentOrCount max_surge = 2;

  // Maximum amount of time that the service will spend on attempting gracefully draining a node (evicting it's pods), before
  // falling back to pod deletion.
  // By default, node can be drained unlimited time.
  // Important consequence of that is if PodDisruptionBudget doesn't allow to evict a pod,
  // then NodeGroup update with node re-creation will hung on that pod eviction.
  // Note, that it is different from `kubectl drain --timeout`
  google.protobuf.Duration drain_timeout = 3 [(buf.validate.field) = { duration: { gte: { } } }];
}

message PercentOrCount {
  oneof value {
    option (buf.validate.oneof).required = true;

    int64 percent = 1 [(buf.validate.field) = { int64: { lte: 100, gte: 0 } }];

    int64 count = 2 [(buf.validate.field) = { int64: { gte: 0 } }];
  }
}

message NodeGroupStatus {
  enum State {
    STATE_UNSPECIFIED = 0;

    PROVISIONING = 1;

    RUNNING = 2;

    DELETING = 3;
  }

  State state = 1;

  // Actual version of NodeGroup. Have format `<major>.<minor>.<patch>-nebius-node.<infra_version>` like "1.30.0-nebius-node.10".
  // Where <major>.<minor>.<patch> is Kubernetes version and <infra_version> is version of Node infrastructure and configuration,
  // which update may include bug fixes, security updates and new features depending on worker node configuration.
  string version = 2;

  // Desired total number of nodes that should be in the node group.
  // It is either `NodeGroupSpec.fixed_node_count` or arbitrary number between
  // `NodeGroupAutoscalingSpec.min_node_count` and `NodeGroupAutoscalingSpec.max_node_count` decided by autoscaler.
  int64 target_node_count = 3;

  // Total number of nodes that are currently in the node group.
  // Both ready and not ready nodes are counted.
  int64 node_count = 4;

  // Total number of nodes that has outdated node configuration.
  // These nodes will be replaced by new nodes with up-to-date configuration.
  int64 outdated_node_count = 5;

  // Total number of nodes that successfully joined the cluster and are ready to serve workloads.
  // Both outdated and up-to-date nodes are counted.
  int64 ready_node_count = 6;

  // Show that there are changes are in flight.
  bool reconciling = 100;
}
