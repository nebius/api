syntax = "proto3";

package nebius.mk8s.v1;

import "buf/validate/validate.proto";
import "google/protobuf/duration.proto";
import "nebius/annotations.proto";
import "nebius/common/v1/metadata.proto";
import "nebius/common/v1/resource_event.proto";
import "nebius/mk8s/v1/instance_template.proto";

option go_package = "github.com/nebius/gosdk/proto/nebius/mk8s/v1";
option java_multiple_files = true;
option java_outer_classname = "NodeGroupProto";
option java_package = "ai.nebius.pub.mk8s.v1";

// NodeGroup represents Kubernetes node pool - set of worker machines having the same configuration.
// A Node is a Nebius Compute Instance created in Cluster.metadata.parent_id container, running kubelet
// that registers in Kubernetes API and a Node object created.
message NodeGroup {
  common.v1.ResourceMetadata metadata = 1; // The parent_id is an ID of Cluster

  NodeGroupSpec spec = 2;

  NodeGroupStatus status = 3;
}

message NodeGroupSpec {
  // Version is desired Kubernetes version of the cluster. For now only acceptable format is
  // `<major>.<minor>` like "1.31". Option for patch version update will be added later.
  // By default the cluster control plane `<major>.<minor>` version will be used.
  string version = 1 [(buf.validate.field) = {
    string: {pattern: "|^\\d\\.\\d\\d$"}
  }];

  oneof size {
    option (buf.validate.oneof).required = true;

    // Number of nodes in the group. Can be changed manually at any time.
    int64 fixed_node_count = 2 [(buf.validate.field) = {
      int64: {
        lte: 100
        gte: 0
      }
    }];

    // Enables [Kubernetes Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)
    // for that NodeGroup, and defines autoscaling parameters.
    NodeGroupAutoscalingSpec autoscaling = 5;
  }

  // Parameters for Kubernetes Node object and Nebius Compute Instance
  // If not written opposite a NodeTemplate field update will cause NodeGroup roll-out according NodeGroupDeploymentStrategy.
  NodeTemplate template = 3 [(buf.validate.field).required = true];

  // Defines deployment - roll-out, or nodes re-creation during configuration change.
  // Allows to setup compromise in roll-out speed, extra resources consumption and workloads disruption.
  NodeGroupDeploymentStrategy strategy = 4;

  // Parameters for nodes auto repair.
  NodeGroupAutoRepairSpec auto_repair = 6;
}

message NodeTemplate {
  NodeMetadataTemplate metadata = 1;

  // Kubernetes Node taints.
  // For now change will not be propagated to existing nodes, so will be applied only to Kubernetes Nodes created after the field change.
  // That behaviour may change later.
  // So, for now you will need to manually set them to existing nodes, if that is needed.
  // Field change will NOT trigger NodeGroup roll out.
  repeated NodeTaint taints = 2 [(buf.validate.field) = {
    repeated: {max_items: 100}
  }];

  // Resources that will have Nebius Compute Instance where Node kubelet will run.
  ResourcesSpec resources = 3 [(buf.validate.field).required = true];

  // Parameters of a Node Nebius Compute Instance boot disk.
  DiskSpec boot_disk = 9 [(field_behavior) = NON_EMPTY_DEFAULT];

  // GPU-related settings.
  GpuSettings gpu_settings = 13;

  // OS version that will be used to create the boot disk of Compute Instances in the NodeGroup.
  // Supported platform / Kubernetes version / OS / driver presets combinations
  // * `gpu-l40s-a`, `gpu-l40s-d`, `gpu-h100-sxm`, `gpu-h200-sxm`, `cpu-e1`, `cpu-e2`, `cpu-d3`:
  //   * `drivers_preset`: `""`
  //     * `version`: 1.30 → `"ubuntu22.04"`
  //     * `version`: 1.31 → `"ubuntu22.04"` (default), `"ubuntu24.04"`
  // * `gpu-l40s-a`, `gpu-l40s-d`, `gpu-h100-sxm`, `gpu-h200-sxm`:
  //   * `drivers_preset`: `"cuda12"` (CUDA 12.4)
  //     * `version`: 1.30, 1.31 → `"ubuntu22.04"`
  //   * `drivers_preset`: `"cuda12.4"`
  //     * `version`: 1.31 → `"ubuntu22.04"`
  //   * `drivers_preset`: `"cuda12.8"`
  //     * `version`: 1.31 → `"ubuntu24.04"`
  // * `gpu-b200-sxm`:
  //   * `drivers_preset`: `""`
  //     * `version`: 1.30, 1.31 → `"ubuntu24.04"`
  //   * `drivers_preset`: `"cuda12"` (CUDA 12.8)
  //     * `version`: 1.30, 1.31 → `"ubuntu24.04"`
  //   * `drivers_preset`: `"cuda12.8"`
  //     * `version`: 1.31 → `"ubuntu24.04"`
  // * `gpu-b200-sxm-a`:
  //   * `drivers_preset`: `""`
  //     * `version`: 1.31 → `"ubuntu24.04"`
  //   * `drivers_preset`: `"cuda12.8"`
  //     * `version`: 1.31 → `"ubuntu24.04"`
  string os = 16;

  // Nebius Compute GPUCluster ID that will be attached to node.
  GpuClusterSpec gpu_cluster = 4;

  repeated NetworkInterfaceTemplate network_interfaces = 5 [(field_behavior) = NON_EMPTY_DEFAULT];

  // Static attachments of Compute Filesystem.
  // Can be used as a workaround, until CSI for Compute Disk and Filesystem will be available.
  repeated AttachedFilesystemSpec filesystems = 7;

  // cloud-init user-data
  // Should contain at least one SSH key.
  string cloud_init_user_data = 6 [(sensitive) = true];

  // the Nebius service account whose credentials will be available on the nodes of the group.
  // With these credentials, it is possible to make `nebius` CLI or public API requests from the nodes
  // without the need for extra authentication.
  // This service account is also used to make requests to container registry.
  //
  // `resource.serviceaccount.issueAccessToken` permission is required to use this field.
  string service_account_id = 10;

  // Configures whether the nodes in the group are preemptible.
  // Set to empty value to enable preemptible nodes.
  PreemptibleSpec preemptible = 15 [(field_behavior) = MEANINGFUL_EMPTY_VALUE];

  // reservation_policy is an interface of the "capacity block" (or "capacity block group") mechanism of Nebius Compute.
  ReservationPolicy reservation_policy = 18;
}

message NodeMetadataTemplate {
  // Kubernetes Node labels.
  // For now change will not be propagated to existing nodes, so will be applied only to Kubernetes Nodes created after the field change.
  // That behaviour may change later.
  // So, for now you will need to manually set them to existing nodes, if that is needed.
  //
  // System labels containing "kubernetes.io" and "k8s.io" will be ignored.
  // Field change will NOT trigger NodeGroup roll out.
  map<string, string> labels = 1 [(buf.validate.field) = {
    map: {max_pairs: 100}
  }];
}

// GPU-related settings.
message GpuSettings {
  // Identifier of the predefined set of drivers included in the ComputeImage deployed on ComputeInstances that are part of the NodeGroup.
  // Supported presets for different platform / Kubernetes version combinations:
  // * `gpu-l40s-a`, `gpu-l40s-d`, `gpu-h100-sxm`, `gpu-h200-sxm`:
  //   * `version`: 1.30 → `"cuda12"` (CUDA 12.4)
  //   * `version`: 1.31 → `"cuda12"` (CUDA 12.4), `"cuda12.4"`, `"cuda12.8"`
  // * `gpu-b200-sxm`:
  //   * `version`: 1.31 → `"cuda12"` (CUDA 12.8), `"cuda12.8"`
  // * `gpu-b200-sxm-a`:
  //   * `version`: 1.31 → `"cuda12.8"`
  string drivers_preset = 1 [(buf.validate.field).required = true];
}

message GpuClusterSpec {
  string id = 1;
}

message NetworkInterfaceTemplate {
  // Parameters for Public IPv4 address associated with the interface.
  // Set to empty value, to enable it.
  PublicIPAddress public_ip_address = 1 [(field_behavior) = MEANINGFUL_EMPTY_VALUE];

  // Nebius VPC Subnet ID that will be attached to a node cloud instance network interface.
  // By default Cluster control plane subnet_id used.
  // Subnet should be located in the same network with control plane.
  string subnet_id = 3 [(field_behavior) = NON_EMPTY_DEFAULT];
}

// Describes a public IP address.
message PublicIPAddress {}

message AttachedFilesystemSpec {
  enum AttachMode {
    UNSPECIFIED = 0;

    READ_ONLY = 1;

    READ_WRITE = 2;
  }

  AttachMode attach_mode = 1 [(buf.validate.field).required = true];

  // Specifies the user-defined identifier, allowing to use it as a device in mount command.
  string mount_tag = 2 [(buf.validate.field).required = true];

  oneof type {
    option (buf.validate.oneof).required = true;

    ExistingFilesystem existing_filesystem = 3;
  }
}

message ExistingFilesystem {
  string id = 1 [(buf.validate.field).required = true];
}

message NodeGroupAutoscalingSpec {
  int64 min_node_count = 1 [(buf.validate.field) = {
    int64: {
      lte: 100
      gte: 0
    }
  }];

  int64 max_node_count = 2 [(buf.validate.field) = {
    int64: {
      lte: 100
      gte: 0
    }
  }];

  option (buf.validate.message) = {
    cel: [
      {
        id: "autoscaling.protovalidate.message"
        message: "min_node_count must be less or equal than max_node_count"
        expression: "this.min_node_count <= this.max_node_count"
      }
    ]
  };
}

// See https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
message NodeTaint {
  string key = 1 [(buf.validate.field).required = true];

  string value = 2 [(buf.validate.field).required = true];

  Effect effect = 3 [(buf.validate.field).required = true];

  enum Effect {
    EFFECT_UNSPECIFIED = 0;

    NO_EXECUTE = 1;

    NO_SCHEDULE = 2;

    PREFER_NO_SCHEDULE = 3;
  }
}

message NodeGroupDeploymentStrategy {
  // The maximum number of nodes that can be simultaneously unavailable during the update process.
  //
  // This value can be specified either as an absolute number (for example 3) or as a percentage of the desired
  // number of nodes (for example 5%).
  //
  // When specified as a percentage, the actual number is calculated by rounding down to the nearest whole number.
  // This value cannot be 0 if `max_surge` is also set to 0.
  //
  // Defaults to 0.
  //
  // Example: If set to 20%, up to 20% of the nodes can be taken offline at once during the update,
  // ensuring that at least 80% of the desired nodes remain operational.
  PercentOrCount max_unavailable = 1;

  // The maximum number of additional nodes that can be provisioned above the desired number of nodes during the update process.
  //
  // This value can be specified either as an absolute number (for example 3) or as a percentage of the desired
  // number of nodes (for example 5%).
  //
  // When specified as a percentage, the actual number is calculated by rounding up to the nearest whole number.
  // This value cannot be 0 if `max_unavailable` is also set to 0.
  //
  // Defaults to 1.
  //
  // Example: If set to 25%, the node group can scale up by an additional 25% during the update,
  // allowing new nodes to be added before old nodes are removed, which helps minimize workload disruption.
  //
  // NOTE:
  //
  // it is user responsibility to ensure that there are enough quota for provision nodes above the desired number.
  // Available quota effectively limits `max_surge`.
  // In case of not enough quota even for one extra node, update operation will hung because of quota exhausted error.
  // Such error will be visible in Operation.progress_data.
  PercentOrCount max_surge = 2;

  // Maximum amount of time that the service will spend on attempting gracefully draining a node (evicting it's pods), before
  // falling back to pod deletion.
  // By default, node can be drained unlimited time.
  // Important consequence of that is if PodDisruptionBudget doesn't allow to evict a pod,
  // then NodeGroup update with node re-creation will hung on that pod eviction.
  // Note, that it is different from `kubectl drain --timeout`
  google.protobuf.Duration drain_timeout = 3 [(buf.validate.field) = {
    duration: {
      gte: {}
    }
  }];
}

message PercentOrCount {
  oneof value {
    option (buf.validate.oneof).required = true;

    int64 percent = 1 [(buf.validate.field) = {
      int64: {
        lte: 100
        gte: 0
      }
    }];

    int64 count = 2 [(buf.validate.field) = {
      int64: {gte: 0}
    }];
  }
}

message NodeGroupAutoRepairSpec {
  // Conditions that determine whether a node should be auto repaired.
  repeated NodeAutoRepairCondition conditions = 4;
}

message NodeAutoRepairCondition {
  // Node condition type.
  string type = 1 [(buf.validate.field).required = true];

  // Node condition status.
  ConditionStatus status = 2;

  oneof trigger {
    option (buf.validate.oneof).required = true;

    // The duration after which the node is automatically repaired if the condition remains in the specified status.
    google.protobuf.Duration timeout = 3;

    // When true, disables the default auto-repair condition rules.
    bool disabled = 4;
  }
}

enum ConditionStatus {
  CONDITION_STATUS_UNSPECIFIED = 0;

  TRUE = 1;

  FALSE = 2;

  UNKNOWN = 3;
}

message PreemptibleSpec {}

// ReservationPolicy is copied as-is from NebiusAPI `compute/v1/instance.proto`.
message ReservationPolicy {
  enum Policy {
    // 1) Will try to launch instance in any reservation_ids if provided.
    // 2) Will try to launch instance in any of the available capacity block.
    // 3) Will try to launch instance in PAYG if 1 & 2 are not satisfied.
    AUTO = 0;

    // The instance is launched only using on-demand (PAYG) capacity.
    // No attempt is made to find or use a Capacity Block.
    // It's an error to provide reservation_ids with policy = FORBID
    FORBID = 1;

    // 1) Will try to launch the instance in Capacity Blocks from reservation_ids if provided.
    // 2) If reservation_ids are not provided will try to launch instance in suitable & available Capacity Block.
    // 3) Fail otherwise.
    STRICT = 2;
  }

  Policy policy = 1;

  // Capacity block groups, order matters
  repeated string reservation_ids = 2;
}

message NodeGroupStatus {
  enum State {
    STATE_UNSPECIFIED = 0;

    PROVISIONING = 1;

    RUNNING = 2;

    DELETING = 3;
  }

  State state = 1;

  // Actual version of NodeGroup. Have format `<major>.<minor>.<patch>-nebius-node.<infra_version>` like "1.30.0-nebius-node.10".
  // Where `<major>.<minor>.<patch>` is Kubernetes version and `<infra_version>` is version of Node infrastructure and configuration,
  // which update may include bug fixes, security updates and new features depending on worker node configuration.
  string version = 2;

  // Desired total number of nodes that should be in the node group.
  // It is either `NodeGroupSpec.fixed_node_count` or arbitrary number between
  // `NodeGroupAutoscalingSpec.min_node_count` and `NodeGroupAutoscalingSpec.max_node_count` decided by autoscaler.
  int64 target_node_count = 3;

  // Total number of nodes that are currently in the node group.
  // Both ready and not ready nodes are counted.
  int64 node_count = 4;

  // Total number of nodes that has outdated node configuration.
  // These nodes will be replaced by new nodes with up-to-date configuration.
  int64 outdated_node_count = 5;

  // Total number of nodes that successfully joined the cluster and are ready to serve workloads.
  // Both outdated and up-to-date nodes are counted.
  int64 ready_node_count = 6;

  repeated common.v1.RecurrentResourceEvent events = 61;

  // Show that there are changes are in flight.
  bool reconciling = 100;
}
