syntax = "proto3";

package nebius.mk8s.v1alpha1;

import "buf/validate/validate.proto";
import "google/protobuf/duration.proto";
import "nebius/annotations.proto";
import "nebius/common/v1/metadata.proto";
import "nebius/mk8s/v1alpha1/instance_template.proto";

// Migrate to v1.
option deprecated = true;
option go_package = "github.com/nebius/gosdk/proto/nebius/mk8s/v1alpha1";
option java_multiple_files = true;
option java_outer_classname = "NodeGroupProto";
option java_package = "ai.nebius.pub.mk8s.v1alpha1";
option (file_deprecation_details) = {description: "migrate to v1"};

// NodeGroup represents Kubernetes node pool
message NodeGroup {
  common.v1.ResourceMetadata metadata = 1; // the parent_id is an ID of Cluster

  NodeGroupSpec spec = 2;

  NodeGroupStatus status = 3;
}

message NodeGroupSpec {
  // Version is desired Kubernetes version of the cluster. For now only acceptable format is
  // `MAJOR.MINOR` like "1.31". Option for patch version update will be added later.
  // By default the cluster control plane MAJOR.MINOR version will be used.
  string version = 1;

  oneof size {
    option (buf.validate.oneof).required = true;

    int64 fixed_node_count = 2 [(buf.validate.field) = {
      int64: {
        lte: 100
        gte: 0
      }
    }];

    NodeGroupAutoscalingSpec autoscaling = 5;
  }

  NodeTemplate template = 3 [(buf.validate.field).required = true];

  NodeGroupDeploymentStrategy strategy = 4;

  // Parameters for nodes auto repair.
  NodeGroupAutoRepairSpec auto_repair = 6;
}

message NodeTemplate {
  NodeMetadataTemplate metadata = 1;

  repeated NodeTaint taints = 2;

  ResourcesSpec resources = 3 [(buf.validate.field).required = true];

  DiskSpec boot_disk = 9 [(field_behavior) = NON_EMPTY_DEFAULT];

  // GPU-related settings.
  GpuSettings gpu_settings = 13;

  // OS version that will be used to create the boot disk of Compute Instances in the NodeGroup.
  // Supported platform / k8s version /  OS / driver presets combinations
  // - gpu-l40s-a, gpu-l40s-d, gpu-h100-sxm, gpu-h200-sxm, cpu-e1, cpu-e2, cpu-d3:
  //   - drivers_preset: ""
  //     - k8s: 1.30 → "ubuntu22.04"
  //     - k8s: 1.31 → "ubuntu22.04" (default), "ubuntu24.04"
  // - gpu-l40s-a, gpu-l40s-d, gpu-h100-sxm, gpu-h200-sxm:
  //   - drivers_preset: "cuda12" (CUDA 12.4)
  //     - k8s: 1.30, 1.31 → "ubuntu22.04"
  //   - drivers_preset: "cuda12.4"
  //     - k8s: 1.31 → "ubuntu22.04"
  //   - drivers_preset: "cuda12.8"
  //     - k8s: 1.31 → "ubuntu24.04"
  // - gpu-b200-sxm:
  //   - drivers_preset: ""
  //     - k8s: 1.30, 1.31 → "ubuntu24.04"
  //   - drivers_preset: "cuda12" (CUDA 12.8)
  //     - k8s: 1.30, 1.31 → "ubuntu24.04"
  //   - drivers_preset: "cuda12.8"
  //     - k8s: 1.31 → "ubuntu24.04"
  // - gpu-b200-sxm-a:
  //   - drivers_preset: ""
  //     - k8s: 1.31 → "ubuntu24.04"
  //   - drivers_preset: "cuda12.8"
  //     - k8s: 1.31 → "ubuntu24.04"
  string os = 16;

  GpuClusterSpec gpu_cluster = 4;

  repeated NetworkInterfaceTemplate network_interfaces = 5 [(field_behavior) = NON_EMPTY_DEFAULT];

  repeated AttachedFilesystemSpec filesystems = 7;

  // cloud-init user-data. Must contain at least one SSH key.
  string cloud_init_user_data = 6 [(sensitive) = true];

  // the Nebius service account whose credentials will be available on the nodes of the group. With these credentials, it is possible to
  // make `nebius` CLI or public API requests from the nodes without the need for extra authentication. This service account is also used to
  // make requests to container registry.
  //
  // `resource.serviceaccount.issueAccessToken` permission is required to use this field.
  string service_account_id = 10;

  // Configures whether the nodes in the group are preemptible.
  // Set to empty value to enable preemptible nodes.
  PreemptibleSpec preemptible = 15 [(field_behavior) = MEANINGFUL_EMPTY_VALUE];
}

message NodeMetadataTemplate {
  // Labels will be propagated into nodes metadata.
  // System labels containing "kubernetes.io" and "k8s.io" will not be propagated.
  // On update labels they will not be updated in nodes right away, only on node group update.
  map<string, string> labels = 1;
}

// GPU-related settings.
message GpuSettings {
  // Identifier of the predefined set of drivers included in the ComputeImage deployed on ComputeInstances that are part of the NodeGroup.
  // Supported presets for different platform / k8s version combinations:
  // - gpu-l40s-a, gpu-l40s-d, gpu-h100-sxm, gpu-h200-sxm:
  //   - k8s: 1.30 → "cuda12" (CUDA 12.4)
  //   - k8s: 1.31 → "cuda12" (CUDA 12.4), "cuda12.4", "cuda12.8"
  // - gpu-b200-sxm:
  //   - k8s: 1.31 → "cuda12" (CUDA 12.8), "cuda12.8"
  // - gpu-b200-sxm-a:
  //   - k8s: 1.31 → "cuda12.8"
  string drivers_preset = 1 [(buf.validate.field).required = true];
}

message GpuClusterSpec {
  string id = 1;
}

message NetworkInterfaceTemplate {
  // Public IPv4 address associated with the interface.
  PublicIPAddress public_ip_address = 1 [(field_behavior) = MEANINGFUL_EMPTY_VALUE];

  // Subnet ID that will be attached to a node cloud instance network interface.
  // By default control plane subnet_id used.
  // Subnet should be located in the same network with control plane and have same parent ID as cluster.
  string subnet_id = 3 [(field_behavior) = NON_EMPTY_DEFAULT];
}

// Describes a public IP address.
message PublicIPAddress {}

message AttachedFilesystemSpec {
  enum AttachMode {
    UNSPECIFIED = 0;

    READ_ONLY = 1;

    READ_WRITE = 2;
  }

  AttachMode attach_mode = 1 [(buf.validate.field).required = true];

  string device_name = 2 [(buf.validate.field).required = true];

  oneof type {
    option (buf.validate.oneof).required = true;

    ExistingFilesystem existing_filesystem = 3;
  }
}

message ExistingFilesystem {
  string id = 1 [(buf.validate.field).required = true];
}

message NodeGroupAutoscalingSpec {
  int64 min_node_count = 1 [(buf.validate.field) = {
    int64: {
      lte: 100
      gte: 0
    }
  }];

  int64 max_node_count = 2 [(buf.validate.field) = {
    int64: {
      lte: 100
      gte: 0
    }
  }];

  option (buf.validate.message) = {
    cel: [
      {
        id: "autoscaling.protovalidate.message"
        message: "min_node_count must be less or equal than max_node_count"
        expression: "this.min_node_count <= this.max_node_count"
      }
    ]
  };
}

message PreemptibleSpec {}

// See https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
message NodeTaint {
  string key = 1 [(buf.validate.field).required = true];

  string value = 2 [(buf.validate.field).required = true];

  Effect effect = 3 [(buf.validate.field).required = true];

  enum Effect {
    EFFECT_UNSPECIFIED = 0;

    NO_EXECUTE = 1;

    NO_SCHEDULE = 2;

    PREFER_NO_SCHEDULE = 3;
  }
}

message NodeGroupDeploymentStrategy {
  // The maximum number of machines that can be unavailable during the update.
  // Value can be an absolute number (ex: 5) or a percentage of desired
  // machines (ex: 10%).
  // Absolute number is calculated from percentage by rounding down.
  // This can not be 0 if MaxSurge is 0.
  // Defaults to 0.
  // Example: when this is set to 30%, the old MachineSet can be scaled
  // down to 70% of desired machines immediately when the rolling update
  // starts. Once new machines are ready, old MachineSet can be scaled
  // down further, followed by scaling up the new MachineSet, ensuring
  // that the total number of machines available at all times
  // during the update is at least 70% of desired machines.
  PercentOrCount max_unavailable = 1;

  // The maximum number of machines that can be scheduled above the
  // desired number of machines.
  // Value can be an absolute number (ex: 5) or a percentage of
  // desired machines (ex: 10%).
  // This can not be 0 if MaxUnavailable is 0.
  // Absolute number is calculated from percentage by rounding up.
  // Defaults to 1.
  // Example: when this is set to 30%, the new MachineSet can be scaled
  // up immediately when the rolling update starts, such that the total
  // number of old and new machines do not exceed 130% of desired
  // machines. Once old machines have been killed, new MachineSet can
  // be scaled up further, ensuring that total number of machines running
  // at any time during the update is at most 130% of desired machines.
  PercentOrCount max_surge = 2;

  // DrainTimeout is the total amount of time that the service will spend on draining a node.
  // By default, node can be drained without any time limitations.
  // NOTE: NodeDrainTimeout is different from `kubectl drain --timeout`
  google.protobuf.Duration drain_timeout = 3;
}

message PercentOrCount {
  oneof value {
    option (buf.validate.oneof).required = true;

    int64 percent = 1;

    int64 count = 2;
  }
}

message NodeGroupAutoRepairSpec {
  // Conditions that determine whether a node should be auto repaired.
  repeated NodeAutoRepairCondition conditions = 4;
}

message NodeAutoRepairCondition {
  // Node condition type.
  string type = 1 [(buf.validate.field).required = true];

  // Node condition status.
  ConditionStatus status = 2;

  oneof trigger {
    option (buf.validate.oneof).required = true;

    // The duration after which the node is automatically repaired if the condition remains in the specified status.
    google.protobuf.Duration timeout = 3;

    // When true, disables the default auto-repair condition rules.
    bool disabled = 4;
  }
}

enum ConditionStatus {
  CONDITION_STATUS_UNSPECIFIED = 0;

  TRUE = 1;

  FALSE = 2;

  UNKNOWN = 3;
}

message NodeGroupStatus {
  enum State {
    STATE_UNSPECIFIED = 0;

    PROVISIONING = 1;

    RUNNING = 2;

    DELETING = 3;
  }

  State state = 1;

  // Version have format `MAJOR.MINOR.PATCH-nebius-node.n` like "1.30.0-nebius-node.10".
  string version = 2;

  // Desired total number of nodes that should be in the node group.
  // It is either fixed_node_count or arbitrary number between min_node_count and max_node_count decided by autoscaler.
  int64 target_node_count = 3;

  // Total number of nodes that are currently in the node group.
  // Both ready and not ready nodes are counted.
  int64 node_count = 4;

  // Total number of nodes that has outdated node configuration.
  // These nodes will be replaced by new nodes with up-to-date configuration.
  int64 outdated_node_count = 5;

  // Total number of nodes that successfully joined the cluster and are ready to serve workloads.
  // Both outdated and up-to-date nodes are counted.
  int64 ready_node_count = 6;

  // Show that changes are in flight
  bool reconciling = 100;
}
